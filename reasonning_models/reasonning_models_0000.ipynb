{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid R-K System: Main Runner\n",
    "\n",
    "This notebook serves as the main entry point for running and demonstrating the Hybrid Reasoning-Knowledge (R-K) system. It imports the necessary components from the `hybrid_rk_merged.py` script, sets up the configuration, and executes a sample query.\n",
    "\n",
    "To run this notebook, you should have already set up your environment as described in the main `README.md` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "The following cells handle the initial setup:\n",
    "1.  **Dependencies**: Installs `openai` and `chromadb` if they are not already present.\n",
    "2.  **Imports**: Imports standard Python libraries.\n",
    "3.  **Working Directory**: **Crucially, you must change the path in this cell** to the directory where the `hybrid_rk_merged.py` file is located.\n",
    "4.  **API Key**: Sets the `OPENAI_API_KEY` environment variable. **You must replace the placeholder with your actual key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec813df-04e9-4161-8ac0-4b9e70a45b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai\n",
    "#%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a725ea-dc20-4e56-b144-f778a16dd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from time import time\n",
    "import json\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477468f5-1058-4e89-9cda-9aa7b9694aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Change this to the directory containing hybrid_rk_merged.py\n",
    "os.chdir(r\"<folder where hybrid_rk_merged.py is >\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7feb0c1c-d359-4930-a04b-7656637d3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"sk-<>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Core System Classes\n",
    "\n",
    "This cell imports all the necessary classes from the `hybrid_rk_merged.py` module, including the LLM adapters, retriever classes, and the main reasoning agents and controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa2dd10-5383-49b0-b1db-47041fdb5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid_rk_merged import OpenAIResponsesAdapter, RAgent, KAgent, HybridController, OpenAIEmbedder, ChromaRetriever , OpenAIChatAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory Knowledge Base (for testing)\n",
    "\n",
    "This is a simple, toy knowledge base that can be used for quick tests without needing to set up a full vector database. The main execution block below uses a more advanced `ChromaRetriever` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94089ea-8449-47e8-a9f1-45dd9c343fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# ==== Simple In-Memory KB + Retriever   ===\n",
    "# ==========================================\n",
    "\n",
    "class InMemoryKB:\n",
    "    \"\"\"A toy KB. Replace with your RAG/vector DB/web search.\"\"\"\n",
    "    def __init__(self, rows: List[Dict[str, Any]]):\n",
    "        self.rows = rows\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        # naive: rank by shared tokens\n",
    "        q_tokens = set(query.lower().split())\n",
    "        scored = []\n",
    "        for r in self.rows:\n",
    "            text = (r.get(\"text\",\"\") + \" \" + r.get(\"meta\",\"\")).lower()\n",
    "            score = len(q_tokens.intersection(set(text.split())))\n",
    "            if score > 0:\n",
    "                scored.append((score, r))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [r for _, r in scored[:k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Execution Block\n",
    "\n",
    "This cell contains the main logic for running the system. It performs the following steps:\n",
    "\n",
    "1.  **Selects the LLM**: Initializes the `OpenAIResponsesAdapter`.\n",
    "2.  **Selects the Knowledge Base**: Chooses between `ChromaRetriever` (default) and `PGVectorRetriever` based on the `RK_STACK` environment variable.\n",
    "3.  **Initializes the Retriever**: Sets up the ChromaDB collection and, if it's the first run, seeds it with sample data.\n",
    "4.  **Initializes Agents**: Creates instances of the `KAgent` (Knowledge Agent) and `RAgent` (Reasoning Agent).\n",
    "5.  **Initializes the Controller**: Creates the main `HybridController`.\n",
    "6.  **Runs the Query**: Calls the `controller.solve()` method with a sample question and prints the results, including the plan, the final answer, and the fact-checking audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77f3efa-c5f4-4242-a810-44b7c42290c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PLAN ===\n",
      "{\n",
      "  \"subquestions\": [\n",
      "    {\n",
      "      \"q\": \"Compare Model A vs Model B for on-device summarization.\",\n",
      "      \"needs_facts\": true\n",
      "    }\n",
      "  ],\n",
      "  \"assumptions\": []\n",
      "}\n",
      "\n",
      "=== FINAL ===\n",
      "Answer:\n",
      "- Model A memory footprint: ~1.8GB int8 quantized; Model B: ~2.3GB int8. (source: kb://models#mem)\n",
      "- Model B license restricts redistribution of weights; on-device inference allowed. (source: kb://models#licenseB)\n",
      "- Model B typical on-device latency is ~120ms for 256 tokens on Snapdragon X. (source: kb://models#B_latency)\n",
      "- Model A license permits on-device commercial use with attribution. (source: kb://models#licenseA)\n",
      "- Model A typical on-device latency is ~80ms for 256 tokens on Snapdragon X. (source: kb://models#A_latency)\n",
      "\n",
      "Groundedness audit:\n",
      "? Answer:  (sources: —)\n",
      "✔ - Model A memory footprint: ~1.8GB int8 quantized; Model B: ~2.3GB int8. (source: kb://models#mem)  (sources: kb://models#mem)\n",
      "✔ - Model B license restricts redistribution of weights; on-device inference allowed. (source: kb://models#licenseB)  (sources: kb://models#licenseB)\n",
      "✔ - Model B typical on-device latency is ~120ms for 256 tokens on Snapdragon X. (source: kb://models#B_latency)  (sources: kb://models#B_latency, kb://models#A_latency)\n",
      "✔ - Model A license permits on-device commercial use with attribution. (source: kb://models#licenseA)  (sources: kb://models#licenseA)\n",
      "✔ - Model A typical on-device latency is ~80ms for 256 tokens on Snapdragon X. (source: kb://models#A_latency)  (sources: kb://models#A_latency, kb://models#B_latency)\n",
      "\n",
      "=== CHECKS ===\n",
      "[\n",
      "  {\n",
      "    \"claim\": \"Answer:\",\n",
      "    \"supported_by\": [],\n",
      "    \"status\": \"uncertain\",\n",
      "    \"suggestion\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"claim\": \"- Model A memory footprint: ~1.8GB int8 quantized; Model B: ~2.3GB int8. (source: kb://models#mem)\",\n",
      "    \"supported_by\": [\n",
      "      \"kb://models#mem\"\n",
      "    ],\n",
      "    \"status\": \"supported\",\n",
      "    \"suggestion\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"claim\": \"- Model B license restricts redistribution of weights; on-device inference allowed. (source: kb://models#licenseB)\",\n",
      "    \"supported_by\": [\n",
      "      \"kb://models#licenseB\"\n",
      "    ],\n",
      "    \"status\": \"supported\",\n",
      "    \"suggestion\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"claim\": \"- Model B typical on-device latency is ~120ms for 256 tokens on Snapdragon X. (source: kb://models#B_latency)\",\n",
      "    \"supported_by\": [\n",
      "      \"kb://models#B_latency\",\n",
      "      \"kb://models#A_latency\"\n",
      "    ],\n",
      "    \"status\": \"supported\",\n",
      "    \"suggestion\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"claim\": \"- Model A license permits on-device commercial use with attribution. (source: kb://models#licenseA)\",\n",
      "    \"supported_by\": [\n",
      "      \"kb://models#licenseA\"\n",
      "    ],\n",
      "    \"status\": \"supported\",\n",
      "    \"suggestion\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"claim\": \"- Model A typical on-device latency is ~80ms for 256 tokens on Snapdragon X. (source: kb://models#A_latency)\",\n",
      "    \"supported_by\": [\n",
      "      \"kb://models#A_latency\",\n",
      "      \"kb://models#B_latency\"\n",
      "    ],\n",
      "    \"status\": \"supported\",\n",
      "    \"suggestion\": \"\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose stack via env: RK_STACK in {\"chroma\",\"pgvector\"}\n",
    "    stack = os.environ.get(\"RK_STACK\", \"chroma\").lower()\n",
    "\n",
    "    # Choose LLM\n",
    "    # Default to OpenAI Responses adapter. Swap to Azure by instantiating AzureOpenAIChatAdapter().\n",
    "    llm = OpenAIResponsesAdapter()             # OpenAIResponsesAdapter(model=os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\"))\n",
    "\n",
    "    # Build retriever + embedder\n",
    "    if stack == \"pgvector\":\n",
    "        dsn = os.environ.get(\"PG_DSN\", \"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "        embedder: Embedder = OpenAIEmbedder()\n",
    "        retr = PGVectorRetriever(dsn=dsn, dim=3072)\n",
    "        # Seed a couple docs idempotently\n",
    "        docs = [\n",
    "            (\"pa\", \"Model A typical on-device latency is ~80ms for 256 tokens on Snapdragon X.\", \"kb://models#A_latency\", \"2025-01-15\"),\n",
    "            (\"pb\", \"Model B typical on-device latency is ~120ms for 256 tokens on Snapdragon X.\", \"kb://models#B_latency\", \"2025-02-10\"),\n",
    "            (\"pm\", \"Model A memory footprint: ~1.8GB int8 quantized; Model B: ~2.3GB int8.\", \"kb://models#mem\", \"2025-02-12\"),\n",
    "        ]\n",
    "        embs = embedder.embed([d[1] for d in docs])\n",
    "        retr.add(((d[0], d[1], d[2], d[3], e) for d, e in zip(docs, embs)))\n",
    "        k_agent = KAgent(retriever=retr, embedder=embedder)\n",
    "    else:\n",
    "        # Chroma\n",
    "        embedder = OpenAIEmbedder()\n",
    "        retr = ChromaRetriever(collection=\"rk_demo\", persist_dir=os.environ.get(\"CHROMA_DIR\", \".chroma\"), embedder=embedder)\n",
    "        if retr.count() == 0:\n",
    "            texts = [\n",
    "                \"Model A typical on-device latency is ~80ms for 256 tokens on Snapdragon X.\",\n",
    "                \"Model B typical on-device latency is ~120ms for 256 tokens on Snapdragon X.\",\n",
    "                \"Model A memory footprint: ~1.8GB int8 quantized; Model B: ~2.3GB int8.\",\n",
    "                \"Model A license permits on-device commercial use with attribution.\",\n",
    "                \"Model B license restricts redistribution of weights; on-device inference allowed.\",\n",
    "            ]\n",
    "            metas = [\n",
    "                {\"source\": \"kb://models#A_latency\", \"date\": \"2025-01-15\"},\n",
    "                {\"source\": \"kb://models#B_latency\", \"date\": \"2025-02-10\"},\n",
    "                {\"source\": \"kb://models#mem\", \"date\": \"2025-02-12\"},\n",
    "                {\"source\": \"kb://models#licenseA\", \"date\": \"2024-11-02\"},\n",
    "                {\"source\": \"kb://models#licenseB\", \"date\": \"2025-03-01\"},\n",
    "            ]\n",
    "            retr.add(ids=[f\"r{i}\" for i in range(len(texts))], texts=texts, metadatas=metas)\n",
    "        k_agent = KAgent(retriever=retr)\n",
    "\n",
    "    r_agent = RAgent(llm=llm)\n",
    "    controller = HybridController(r=r_agent, k=k_agent, enable_critic_pass=True, use_k_based_checker=True)\n",
    "\n",
    "    question = os.environ.get(\"RK_QUESTION\", \"Compare Model A vs Model B for on-device summarization.\")\n",
    "    result = controller.solve(question)\n",
    "\n",
    "    print(\"\\n=== PLAN ===\")\n",
    "    print(json.dumps(result[\"plan\"], indent=2))\n",
    "    print(\"\\n=== FINAL ===\")\n",
    "    print(result[\"final\"]) \n",
    "    print(\"\\n=== CHECKS ===\")\n",
    "    print(json.dumps(result[\"checks\"], indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
